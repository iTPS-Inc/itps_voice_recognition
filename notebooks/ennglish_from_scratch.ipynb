{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","import json\n","import os\n","import pickle\n","import pprint\n","from functools import lru_cache\n","from pathlib import Path\n","\n","import jiwer\n","import torchaudio\n","import torchaudio.transforms as T\n","from datasets import load_dataset\n","from dsets.dset_config.dset_config import DatasetConfig\n","from dsets.dsets import ENGLISH_DATASETS, get_datasets\n","from dsets.helpers.helpers import apply_parallel\n","from fastai.callback.tensorboard import TensorBoardCallback\n","from fastai.data.all import *\n","from fastai.callback.all import SaveModelCallback\n","from fastai.text.all import *\n","from fastai.vision.all import *\n","from itpsaudio.aug_transforms import AddNoise, RandomReverbration\n","from itpsaudio.core import *\n","from itpsaudio.transforms import *\n","from transformers import AutoModelForCTC, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC\n","\n","TEST_RUN=False\n","NUM_EPOCHS=1\n","\n","datadir = Path(\"../../data/\")\n","modeldir = datadir / \"models\" / \"audio_en\"\n","\n","# pretrained_model_name = \"facebook/wav2vec2-xls-r-300m\"\n","# pretrained_model_name = \"facebook/wav2vec2-base\"\n","pretrained_model_name = \"OthmaneJ/distil-wav2vec2\"\n","pretrained_model_save_name = pretrained_model_name.replace(\"/\", \"_\")\n","\n","pretrained_save_path = modeldir / pretrained_model_save_name\n","logdir = datadir / \"logs\" /\"audio_en\" / pretrained_model_save_name\n","\n","en_vocab = \"../../notebooks/assets/en_vocab.json\"\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["datasets = [\n","    # DatasetConfig(name='itps', split='train', lang='en', kind=None),\n","    DatasetConfig(name='librispeech', split='dev', lang=None, kind='clean'),\n","    # DatasetConfig(name='librispeech', split='train', lang=None, kind='clean'),\n","    # DatasetConfig(name='librispeech', split='train', lang=None, kind='other'),\n","    DatasetConfig(name='ljl', split='train', lang=None, kind=None),\n","    DatasetConfig(name='nict_spreds', split='train', lang='en', kind=None)\n"," ]\n","\n","if TEST_RUN:\n","  if datasets == []:\n","    p, df = get_datasets([ENGLISH_DATASETS[0]])\n","  else:\n","    p, df = get_datasets([datasets[0]])\n","else:\n","  if datasets == []:\n","    df = load_dataset(\"common_voice\", \"en\",split=\"train\")\n","  else:\n","    p, df = get_datasets(datasets)\n","if os.path.exists(\"df.pkl\"):\n","  df = pd.read_pickle(\"df.pkl\")\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","@lru_cache(maxsize=None)\n","def get_audio_length(s):\n","  t, sr = torchaudio.load(s)\n","  return len(t[0])/sr\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","wav2vec2tok = Wav2Vec2CTCTokenizer(en_vocab,bos_token=\"[BOS]\",\n","                                   eos_token=\"[EOS]\",\n","                                   unk_token=\"[UNK]\",\n","                                   pad_token=\"[PAD]\",\n","                                   )\n","tok = ENTransformersTokenizer(tok=wav2vec2tok)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not \"audio_length\" in df.columns:\n","  df[\"audio_length\"] = df[\"filename\"].apply(get_audio_length).copy()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"audio_length\"].max()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"audio_length\"].plot.hist()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"audio_length\"].sum() / 60 / 60\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MAX_AUDIO_LENGTH=15\n","df = df[df[\"audio_length\"]<MAX_AUDIO_LENGTH].reset_index(drop=True)\n","df = df[~df[\"text\"].isna()].reset_index(drop=True)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"audio_length\"].sum() / 60 / 60\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["splits=RandomSplitter(valid_pct=0.2)(df)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tfms = TfmdLists(df, AudioBatchTransform(), splits=splits)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def load_t_model(mod_path,\n","        attention_dropout=0.08,\n","        hidden_dropout=0.08,\n","        feat_proj_dropout=0.08,\n","        mask_time_prob=0.05,\n","        mask_feature_prob=0.05,\n","        layerdrop=0.08,\n","        ctc_zero_infinity=True,\n","        pad_token_id=tok.tokenizer.pad_token_id,\n","        vocab_size=len(tok.tokenizer),\n","        **kwargs\n","):\n","    return Wav2Vec2ForCTC.from_pretrained(\n","        mod_path,\n","        attention_dropout=attention_dropout,\n","        hidden_dropout=hidden_dropout,\n","        feat_proj_dropout=feat_proj_dropout,\n","        mask_time_prob=mask_time_prob,\n","        mask_feature_prob=mask_feature_prob,\n","        layerdrop=layerdrop,\n","        ctc_zero_infinity=ctc_zero_infinity,\n","        pad_token_id=pad_token_id,\n","        vocab_size=vocab_size,\n","        **kwargs,\n","    )\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","@lru_cache(maxsize=None)\n","def get_sr(x):\n","    _, sr=torchaudio.load(x)\n","    return sr\n","\n","if not \"sr\" in df.columns:\n","  df[\"sr\"] = df[\"filename\"].apply(get_sr)\n","  df.to_pickle(\"df.pkl\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"sr\"].unique()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n","\n","with open(\"noise_sample.wav\", \"wb\") as f:\n","    r = requests.get(SAMPLE_NOISE_URL)\n","    f.write(r.content)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_text_lens = df.loc[splits[0], \"audio_length\"].to_list()\n","val_text_lens = df.loc[splits[1], \"audio_length\"].to_list()\n","srtd_dl=partial(SortedDL, res = train_text_lens)\n","\n","dl_kwargs = [{},{'val_res': val_text_lens}]\n","\n","noise, sr = torchaudio.load(\"noise_sample.wav\")\n","noise = TensorAudio(noise, sr=sr)\n","noise_t = AddNoise(range(1, 10), noise, p=1)\n","\n","resampler = dict()\n","resampler[48000] = T.Resample(48000, 16000)\n","resampler[16000] = noop\n","resampler[22050] = T.Resample(22050, 16000)\n","for sr in df[\"sr\"].unique():\n","    resampler[int(sr)] = T.Resample(sr, 16000)\n","\n","@Transform\n","def resample(x: TensorAudio):\n","    sr = x.sr\n","    if not sr in resampler.keys():\n","        resampler[sr] = T.Resample(sr, 16000)\n","    return TensorAudio(resampler[sr](x), sr=16000)\n","\n","class Range():\n","  def __init__(self, start, stop):\n","    self.start,self.stop=start, stop\n","\n","dls = tfms.dataloaders(bs=2,\n","                        after_item=[RandomReverbration(p=0.2),\n","                                    AddNoise(Range(5, 10), noise,power=3, p=0.2),\n","                                    resample,\n","                                    tok,\n","                                    ],\n","                        # TEXT HAS TO BE PADDED WITH -100 WHEN USING TRANSFORMERS LOSS\n","                        # AUDIO CAN BE ANYTHING\n","                        before_batch=[Pad_Audio_Batch(pad_idx_audio=0,\n","                                                      pad_idx_text=tok.tokenizer.pad_token_id,\n","                                                      pad_first=True,\n","                                                      seq_len=1),\n","                                       squeeze,\n","                                      ],\n","                        shuffle=True,\n","                        n_inp=1,\n","                        dl_type=srtd_dl,\n","                        dl_kwargs=dl_kwargs\n","                       )\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dls.one_batch()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dls.show_batch(tok=tok, unique=False)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def wer(pred, labels):\n","    pred_logits = pred.logits\n","    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)\n","    pred_str = tok.batch_decode(pred_ids)\n","    label_str = tok.batch_decode(labels)\n","    wer = jiwer.wer(label_str, pred_str)\n","    return wer\n","\n","def cer(pred, labels):\n","    pred_logits = pred.logits\n","    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)\n","    pred_str = tok.batch_decode(pred_ids)\n","    label_str = tok.batch_decode(labels)\n","    cer = jiwer.cer(label_str, pred_str)\n","    return cer\n","\n","\n","class TransformersLearner(Learner):\n","    def _do_one_batch(self):\n","        self.pred = self.model(self.xb[0], labels=cast(self.yb[0], torch.Tensor))\n","        self('after_pred')\n","        self.loss_grad = self.pred[\"loss\"]\n","        self.loss = self.loss_grad.clone()\n","        self.smooth_loss = self.loss_grad.clone()\n","        self('after_loss')\n","        if not self.training or not len(self.yb): return\n","        self('before_backward')\n","        self.loss_grad.backward()\n","        self._with_events(self.opt.step, 'step', CancelStepException)\n","        self.opt.zero_grad()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cbs=[TensorBoardCallback(log_dir=logdir,trace_model=False,log_preds=False),\n","     SaveModelCallback(comp=np.less, monitor=\"cer\", fname=modeldir / \"save_model_cb\"), \n","     ]\n","\n","metrics = [Perplexity(), wer,cer]\n","learn = TransformersLearner(dls, load_t_model(pretrained_model_name),\n","                loss_func=noop, # Loss is calculated in Transformers internally\n","                metrics=metrics,\n","                cbs=cbs)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","torch.cuda.empty_cache()\n","gc.collect()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dls.one_batch()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dls.show_batch(tok=tok)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start_lr=1e-7\n","end_lr=10\n","r = learn.lr_find(start_lr=start_lr,\n","                  end_lr=end_lr,\n","                  num_it=100,\n","                  stop_div=True,\n","                  suggest_funcs=())\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def save_model(learn, pretrained_save_path):\n","  pretrained_save_path=\"/content/drive/MyDrive/data/models/wav2vecaug_pre_300m\"\n","  learn.model.save_pretrained(pretrained_save_path)\n","  with open(Path(pretrained_save_path) / \"en_vocab.json\", \"w\") as f:\n","      json.dump(tok.tokenizer.get_vocab(), f)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","if TEST_RUN:\n","  learn.fit_one_cycle(1,1e-3)\n","else:\n","  learn.model.freeze_feature_extractor()\n","  learn.fit_one_cycle(NUM_EPOCHS, lr_max=1e-4, cbs=cbs)\n","  save_model(learn, pretrained_save_path)\n","#   learn.model = load_model(pretrained_save_path)\n","#   learn.fit_one_cycle(1, lr_max=1e-4,)\n","#   save_model(learn, pretrained_save_path)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if TEST_RUN:\n","  test_datasets = [\n","      DatasetConfig(name='librispeech', split='test', lang=None, kind='clean'),\n","      # DatasetConfig(name='librispeech', split='test', lang=None, kind='other'),\n","      # DatasetConfig(name='ljl', split='test', lang=None, kind=None),\n","      # DatasetConfig(name='nict_spreds', split='test', lang='en', kind=None¡£)\n","  ]\n","else:\n","  test_datasets = [\n","      DatasetConfig(name='librispeech', split='test', lang=None, kind='clean'),\n","      # DatasetConfig(name='librispeech', split='test', lang=None, kind='other'),\n","      # DatasetConfig(name='ljl', split='test', lang=None, kind=None),\n","      # DatasetConfig(name='nict_spreds', split='test', lang='en', kind=None)\n","  ]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tp, tdf = get_datasets(test_datasets)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdf[\"audio_length\"] = apply_parallel(tdf[\"filename\"], get_audio_length, 16)\n","tdf = tdf[tdf[\"audio_length\"]<15].reset_index(drop=True)\n","tdf = tdf[~tdf[\"text\"].isna()].reset_index(drop=True)\n","\n","abt = AudioBatchTransform()\n","t_tfms = TfmdLists(tdf, abt)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if TEST_RUN:\n","  t_tfms = TfmdLists(tdf.iloc[:100], abt)\n","else:\n","  t_tfms = TfmdLists(tdf, abt)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t_dl = dls.new(t_tfms)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learn.cbs\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learn.remove_cb(learn.cbs[4]).validate(dl=t_dl)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_preds(xs):\n","  preds=learn.model(xs)\n","  pred_logits=preds.logits\n","  pred_ids=TensorText(np.argmax(pred_logits.detach().cpu().numpy(), axis=-1))\n","  pred_str = tok.batch_decode(pred_ids)\n","  return pred_str\n","  \n","for xs, y in iter(t_dl):\n","  print(wer(learn.model(xs), y))\n","  print(cer(learn.model(xs), y))\n","  pprint.pprint(dict(enumerate(list(zip(get_preds(xs), tok.batch_decode(y))))))\n","  break\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comp = [(get_preds(xs), tok.batch_decode(y)) for xs, y in iter(t_dl)]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, (x_pair, y_pair) in enumerate(comp):\n","  print(\"Pred: \", x_pair[0])\n","  print(\"Targ: \", y_pair[0])\n","  print(\"Pred: \", x_pair[1])\n","  print(\"Targ: \", y_pair[1])\n","  if (i+1 % 10) == 0:\n","    break\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not TEST_RUN:\n","  learn.save(\"/content/drive/MyDrive/data/models/audio_en/export\")\n","  with open(\"/content/drive/MyDrive/data/models/audio_en/export_tokenizer.pkl\", \"wb\") as f:\n","    pickle.dump(tok, f)\n","  learn.model.save_pretrained(\"/content/drive/MyDrive/data/models/audio_en/\")\n","  torch.save(learn.model, \"/content/drive/MyDrive/data/models/audio_en/export_torch_model.pth\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# neptune.stop()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}