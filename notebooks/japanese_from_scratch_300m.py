# -*- coding: utf-8 -*-
"""japanese_from_scratch_300m.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10R67rBQKgkqfBWExCG0n4VFZdq4groLF

# Setup
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %%bash 
# if [ -d itps_voice_recognition ]; then
#   cd itps_voice_recognition
#   git pull -r 
#   cd ..
# else 
#   git clone https://github.com/iTPS-Inc/itps_voice_recognition.git
# fi
# 
# if [ ! -f installed ]; then 
# 
# 
# pip install boto3
# pip install datasets==1.13.3
# pip install transformers==4.11.3
# pip install librosa
# pip install jiwer
# pip install sentencepiece
# pip install japanize_matplotlib
# pip install -Uqq fastai 
# pip install neptune-client
# apt-get -q -y install mecab file libmecab-dev mecab-ipadic-utf8 git curl python-mecab
# git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git 
# echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n  -y
# ln -s /etc/mecabrc /usr/local/etc/mecabrc
# pip install mecab-python3
# 
# touch installed
# fi

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %load_ext tensorboard
# %autoreload 2
import os 
os.chdir("itps_voice_recognition")
from dsets.dsets import ENGLISH_DATASETS, get_datasets, JAPANESE_DATASETS
from dsets.dset_config.dset_config import DatasetConfig

from dsets.helpers.helpers import apply_parallel, get_sampling_rates
from itpsaudio.transforms import * 
from itpsaudio.core import * 
os.chdir("..")

"""# Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir '/content/drive/MyDrive/data/logs/audio_jp'

"""# Imports

## Imports
"""

from fastai.data.all import * 
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForMaskedLM, AutoTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC
from datasets import load_dataset
from fastai.text.all import * 
from fastai.vision.all import *
from fastai.callback.tensorboard import *
from fastai.callback.neptune import *
import torchaudio
import pandas
import MeCab
from pathlib import Path
from IPython.display import Audio, display
import japanize_matplotlib
import jiwer
import json
import pickle
from datasets import load_metric
import neptune.new as neptune

"""## Parameters"""

dset_lengths = None
TEST_RUN=False
NUM_EPOCHS=50
pretrained_model_name = "facebook/wav2vec2-xls-r-300m"
pretrained_model_name = "facebook/hubert-base-ls960"
modelpath=Path("/content/drive/MyDrive/data/models/audio_jp/")
model_out_path = f"/content/drive/MyDrive/data/models/audio_jp/{pretrained_model_name.replace('/', '_')}_{NUM_EPOCHS}_ft"
logdir = Path(f"/content/drive/MyDrive/data/logs/audio_jp/{pretrained_model_name.replace('/', '_')}_{NUM_EPOCHS}_ft/")

if not os.path.exists(model_out_path):
  print("Using Hugginface model model as base")
  pretrained_model_name = "facebook/wav2vec2-xls-r-300m"
  pretrained_model_name = "facebook/wav2vec2-xls-r-300m"
else:
  print("Using older model as base")
  pretrained_model_name = model_out_path

df = load_dataset("common_voice", "ja",split="train")

jsut = DatasetConfig(name="jsut",split="train")
# datasets= [jsut]
datasets = JAPANESE_DATASETS[1:]
p, df1 = get_datasets(datasets)

df = pd.DataFrame({"filename": pd.Series(df["path"]), "text": pd.Series(df["sentence"])})

df = pd.concat([df, df1[["filename", "text"]]], ignore_index=True)
if TEST_RUN:
  df = df.iloc[:100]

from collection import Counter
enumerate(Counter("|".join(df["text"].to_list())).keys()))

"""# Model Training"""

from functools import lru_cache
@lru_cache(maxsize=None)
def get_audio_length(s):
  t, sr = torchaudio.load(s)
  return len(t[0])/sr

from transformers import Wav2Vec2CTCTokenizer
jp_vocab = "/content/drive/MyDrive/data/models/audio_jp/vocab_file.json"
wav2vec2tok = Wav2Vec2CTCTokenizer(jp_vocab,
                                   bos_token="[BOS]",
                                   eos_token="[EOS]",
                                   unk_token="[UNK]",
                                   )
tok = JPTransformersTokenizer(tok=wav2vec2tok)

tok.decode(tok("こんにちは私の名前はじやんです（笑）"),)

df["audio_length"] = df["filename"].apply(get_audio_length)

df["audio_length"].max()

df["audio_length"].plot.hist()

df["audio_length"].sum() / 60 / 60

df = df[df["audio_length"]<15].reset_index(drop=True)
df = df[~df["text"].isna()].reset_index(drop=True)

df["audio_length"].sum() / 60 / 60

splits=RandomSplitter(valid_pct=0.2)(df)

tfms = TfmdLists(df, AudioBatchTransform(), splits=splits)

model = Wav2Vec2ForCTC.from_pretrained(
    # "/content/drive/MyDrive/data/models/audio_jp/",
    pretrained_model_name,
    attention_dropout=0.08,
    hidden_dropout=0.08,
    feat_proj_dropout=0.08,
    mask_time_prob=0.05,
    mask_feature_prob=0.05,
    layerdrop=0.08,
    # ctc_loss_reduction="mean", 
    ctc_zero_infinity=True,
    pad_token_id=tok.tokenizer.pad_token_id,
    vocab_size=len(tok.tokenizer),
).cuda()

from functools import lru_cache

@lru_cache(maxsize=None)
def get_sr(x):
    _, sr=torchaudio.load(x)
    return sr

df["sr"] = df["filename"].apply(get_sr)

df["sr"].unique()

for i, (x, y) in enumerate(iter(learn.dls.valid)):
  print(i)
  with torch.no_grad():
    predss = np.argmax(model(x).logits.detach().cpu(), axis=-1)
    print(tok.batch_decode(predss))
    if i % 3 == 0:
      break
    print(model())
  tok.batch_decode(dls.valid.one_batch()[1])

class NeptuneCallback(Callback):
    "Log losses, metrics, model weights, model architecture summary to neptune"
    order = Recorder.order+1
    def __init__(self, log_model_weights=True, keep_experiment_running=False):
        self.log_model_weights = log_model_weights
        self.keep_experiment_running = keep_experiment_running
        self.experiment = None

        if neptune.project is None:
            raise ValueError('You did not initialize project in neptune.\n',
                             'Please invoke `neptune.init("USERNAME/PROJECT_NAME")` before this callback.')

    def _get\pr
    def before_fit(self):
        try:
            self.experiment = neptune.get_experiment()
        except ValueError:
            print('No active experiment. Please invoke `neptune.create_experiment()` before this callback.')

        try:
            self.experiment.set_property('n_epoch', str(self.learn.n_epoch))
            self.experiment.set_property('model_class', str(type(self.learn.model)))
        except: print(f'Did not log all properties. Check properties in the {neptune.get_experiment()}.')

        try:
            with tempfile.NamedTemporaryFile(mode='w') as f:
                with open(f.name, 'w') as g: g.write(repr(self.learn.model))
                self.experiment.log_artifact(f.name, 'model_summary.txt')
        except: print('Did not log model summary. Check if your model is PyTorch model.')

        if self.log_model_weights and not hasattr(self.learn, 'save_model'):
            print('Unable to log model to Neptune.\n',
                  'Use "SaveModelCallback" to save model checkpoints that will be logged to Neptune.')

    def after_batch(self):
        # log loss and opt.hypers
        if self.learn.training:
            self.experiment.log_metric('batch__smooth_loss', self.learn.smooth_loss)
            self.experiment.log_metric('batch__loss', self.learn.loss)
            self.experiment.log_metric('batch__train_iter', self.learn.train_iter)
            for i, h in enumerate(self.learn.opt.hypers):
                for k, v in h.items(): self.experiment.log_metric(f'batch__opt.hypers.{k}', v)

    def after_epoch(self):
        # log metrics
        for n, v in zip(self.learn.recorder.metric_names, self.learn.recorder.log):
            if n not in ['epoch', 'time']: self.experiment.log_metric(f'epoch__{n}', v)
            if n == 'time': self.experiment.log_text(f'epoch__{n}', str(v))

        # log model weights
        if self.log_model_weights and hasattr(self.learn, 'save_model'):
            if self.learn.save_model.every_epoch:
                _file = join_path_file(f'{self.learn.save_model.fname}_{self.learn.save_model.epoch}',
                                       self.learn.path / self.learn.model_dir, ext='.pth')
            else:
                _file = join_path_file(self.learn.save_model.fname,
                                       self.learn.path / self.learn.model_dir, ext='.pth')
            self.experiment.log_artifact(_file)

    def after_batch(self):
            # log loss and opt.hypers
        if self.learn.training:
            self.experiment.log_metric('batch__smooth_loss', self.learn.smooth_loss)
            self.experiment.log_metric('batch__loss', self.learn.loss)
            self.experiment.log_metric('batch__train_iter', self.learn.train_iter)
            for i, h in enumerate(self.learn.opt.hypers):
                for k, v in h.items(): self.experiment.log_metric(f'batch__opt.hypers.{k}', v)
        # log model weights
        if self.log_model_weights and hasattr(self.learn, 'save_model'):
            if self.learn.save_model.every_epoch:
                _file = join_path_file(f'{self.learn.save_model.fname}_{self.learn.save_model.epoch}',
                                       self.learn.path / self.learn.model_dir, ext='.pth')
            else:
                _file = join_path_file(self.learn.save_model.fname,
                                       self.learn.path / self.learn.model_dir, ext='.pth')
            self.experiment.log_artifact(_file)

    def after_fit(self):
        if not self.keep_experiment_running:
            try: self.experiment.stop()
            except: print('No neptune experiment to stop.')
        else:
            print(f'Your experiment (id: {self.experiment.id}, name: {self.experiment.name}) is left in the running state.\n',
                  'You can log more data to it, like this: `neptune.log_metric()`')

import torchaudio.transforms as T
from itpsaudio.aug_transforms import AddNoise, RandomReverbration
from itpsaudio.callbacks import MixedPrecisionTransformers
train_text_lens = df.loc[splits[0], "audio_length"].to_list()
val_text_lens = df.loc[splits[1], "audio_length"].to_list()
srtd_dl=partial(SortedDL, res = train_text_lens)
dl_kwargs = [{},{'val_res': val_text_lens}]

_res = T.Resample(48000, 16000)

@Transform
def resample(x: TensorAudio):
    if x.sr == 48000:
        return TensorAudio(_res(x), sr=16000)
    elif x.sr == 16000:
        return x

dls = tfms.dataloaders(bs=2,
                        after_item=[RandomReverbration(p=0.1), resample, tok],
                        before_batch=[Pad_Audio_Batch(pad_idx_audio=0,
                                                      pad_idx_text=-100,
                                                      pad_first=True,
                                                      seq_len=1),
                                       squeeze,
                                      ],
                        shuffle=True,
                        n_inp=1,
                        dl_type=srtd_dl,
                        dl_kwargs=dl_kwargs
                       )

dls.one_batch()

dls.show_batch(tok=tok, unique=False)

def wer(pred, labels):
    pred_logits = pred.logits
    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)
    pred_str = tok.batch_decode(pred_ids)
    label_str = tok.batch_decode(labels)
    wer = jiwer.wer(label_str, pred_str)
    return wer

def cer(pred, labels):
    pred_logits = pred.logits
    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)
    pred_str = tok.batch_decode(pred_ids)
    label_str = tok.batch_decode(labels)
    cer = jiwer.cer(label_str, pred_str)
    return cer


class TransformersLearner(Learner):
    def _do_one_batch(self):
        self.pred = self.model(self.xb[0], labels=cast(self.yb[0], torch.Tensor))
        self('after_pred')
        self.loss_grad = self.pred["loss"]
        self.loss = self.loss_grad.clone()
        self.smooth_loss = self.loss_grad.clone()
        self('after_loss')
        if not self.training or not len(self.yb): return
        self('before_backward')
        self.loss_grad.backward()
        self._with_events(self.opt.step, 'step', CancelStepException)
        self.opt.zero_grad()

# Commented out IPython magic to ensure Python compatibility.
# %env NEPTUNE_API_TOKEN='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1YjMyZmIzZS0zNTEyLTQwNmMtYjUxNy05ZWI5NGY1MjQxMDYifQ=='
NEPTUNE=False

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

TensorBoardBaseCallback??

TensorBoardCallback??

Neptune

if NEPTUNE:
  neptune.init(
    project="keisuke.fukuda08/stt-component",
    api_token="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1YjMyZmIzZS0zNTEyLTQwNmMtYjUxNy05ZWI5NGY1MjQxMDYifQ==",
    )

class SeePreds(TensorBoardBaseCallback):
    def __init__(self, n_iters=50, log_dir=None):
       super().__init__()
       store_attr()

    def before_fit(self): self._setup_writer()
    def after_pred(self):
        if self.iter % self.n_iters == 0:
            preds = np.argmax(self.pred.logits.detach().cpu(), axis=-1)
            decoded_preds = tok.batch_decode(preds)
            decoded_ys = tok.batch_decode(self.yb[0])
            print("Preds: ", decoded_preds)
            print("Target: ", decoded_ys)
            for x in self.xb[0]:
              self.writer.add_audio(f"{self.iter}: Audio", x, self.iter, 16000)
            for p,y in zip(decoded_preds, decoded_ys):
              self.writer.add_text(f"{pretrained_model_name}/Prediction",f"Targets:{y}\nPredictions:{p}",  self.iter)
            
cbs=[TensorBoardCallback(log_dir=logdir,trace_model=False,log_preds=False),
     SeePreds(500, log_dir=logdir),
     SaveModelCallback(comp=np.less,
                       monitor="wer",
                       every_epoch=True,
                       at_end=False,
                       fname=Path(modelpath/"wav2vec2_cb")), 
      # MixedPrecisionTransformers(),
     ]

metrics = [Perplexity(), wer,cer]
learn = TransformersLearner(dls, model,
                loss_func=noop, # Loss is calculated in Transformers internally
                metrics=metrics,
                cbs=cbs)

import gc
torch.cuda.empty_cache()
gc.collect()

from torch.utils.tensorboard.writer import SummaryWriter

start_lr=1e-7
end_lr=10
r = learn.lr_find(start_lr=start_lr,
                  end_lr=end_lr,
                  num_it=100,
                  stop_div=True,
                  suggest_funcs=())

model.freeze_feature_extractor()
if NEPTUNE:
  cbs = [NeptuneCallback(log_model_weights=False)]
  neptune.create_experiment()

if TEST_RUN:
  learn.fit_one_cycle(1, lr_max=1e-4, cbs=cbs)
else:
  learn.fit_one_cycle(NUM_EPOCHS, lr_max=1e-4)

  if not os.path.exists(model_out_path):
    os.mkdir(model_out_path)
  learn.save(Path(model_out_path) / "learn_cb", with_opt=True)

if not TEST_RUN:
  learn.model.save_pretrained(model_out_path)
  learn.save(Path(model_out_path) / "learner.falearner")
  with open(Path(model_out_path) / "vocab.json", "w") as f:
    json.dump(tok.tokenizer.get_vocab(), f)
  torch.save(learn.model, Path(model_out_path) / "raw_mod_exp.pth")

test_datasets = [DatasetConfig(name='jsut', split='test', lang=None, kind=None),
                DatasetConfig(name='nict_spreds', split='test', lang='jp', kind=None)]
tp, tdf = get_datasets(test_datasets)

tdf["audio_length"] = apply_parallel(tdf["filename"], get_audio_length, 16)
tdf = tdf[tdf["audio_length"]<15].reset_index(drop=True)
tdf = tdf[~tdf["text"].isna()].reset_index(drop=True)

abt = AudioBatchTransform()
t_tfms = TfmdLists(tdf, abt)

if TEST_RUN:
  t_tfms = TfmdLists(tdf.iloc[:100], abt)
else:
  t_tfms = TfmdLists(tdf, abt)

t_dl = dls.new(t_tfms)

learn.remove_cb(learn.cbs[4]).validate(dl=t_dl)

def get_preds(xs):
  preds=learn.model(xs)
  pred_logits=preds.logits
  pred_ids=TensorText(np.argmax(pred_logits.detach().cpu().numpy(), axis=-1))
  pred_str = tok.batch_decode(pred_ids)
  return pred_str

import pprint

for xs, y in iter(t_dl):
  print(wer(learn.model(xs), y))
  print(cer(learn.model(xs), y))
  pprint.pprint(dict(enumerate(list(zip(get_preds(xs), tok.batch_decode(y))))))
  break

comp = [(get_preds(xs), tok.batch_decode(y)) for xs, y in iter(t_dl)]

for i, (x_pair, y_pair) in enumerate(comp):
  
  print("Pred: ", x_pair[0])
  print("Targ: ", y_pair[0])
  print("Pred: ", x_pair[1])
  print("Targ: ", y_pair[1])
  if (i+1 % 10) == 0:
    break

with open("/content/drive/MyDrive/data/models/audio_jp/good_jp_mod/tok.pkl", "rb") as f:
  t = pickle.load(f)

