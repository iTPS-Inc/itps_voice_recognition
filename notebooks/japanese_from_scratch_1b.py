# -*- coding: utf-8 -*-
# %% 
"""japanese_from_scratch_300m.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10R67rBQKgkqfBWExCG0n4VFZdq4groLF

# Setup
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %%bash
# if [ -d itps_voice_recognition ]; then
#   cd itps_voice_recognition
#   git pull -r
#   cd ..
# else
#   git clone https://github.com/iTPS-Inc/itps_voice_recognition.git
# fi
#
# if [ ! -f installed ]; then
#
#
# apt-get -q -y install mecab file libmecab-dev mecab-ipadic-utf8 git curl
# git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git
# echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n  -y
# ln -s /etc/mecabrc /usr/local/etc/mecabrc
#
# pip install boto3 datasets==1.13.3 transformers==4.11.3 librosa jiwer sentencepiece japanize_matplotlib fastai  neptune-client mecab-python3
#
# touch installed
# fi

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %load_ext tensorboard
# %autoreload 2
# %% 
import os

os.chdir("/raid/gdep-guest33/jp_mod/itps_voice_recognition")
from dsets.dsets import get_datasets, JAPANESE_DATASETS
from dsets.dset_config.dset_config import DatasetConfig
from dsets.helpers.helpers import apply_parallel
from itpsaudio.transforms import *
from functools import lru_cache
from itpsaudio.callbacks import SeePreds, NeptuneSaveModel
from itpsaudio.core import *
# %%

"""# Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir '/content/drive/MyDrive/data/logs/audio_jp'

"""# Imports

## Imports
"""

from fastai.data.all import *
from fastai.distributed import *
from transformers import Wav2Vec2ForCTC
from datasets import load_dataset
from fastai.text.all import *
from fastai.vision.all import *
from fastai.callback.tensorboard import *
from fastai.callback.neptune import *

import torchaudio.transforms as T
from itpsaudio.aug_transforms import RandomReverbration
from functools import lru_cache
import torchaudio
import MeCab
from pathlib import Path
import jiwer
import json
import pickle
import neptune as neptune
import datetime
from tqdm import tqdm

import pprint
# %%
"""## Parameters"""


dset_lengths = None
TEST_RUN = False
LAST_EPOCH = 0
NUM_EPOCHS = 50
FREEZE_FEAT = True

pretrained_model_name = "facebook/wav2vec2-xls-r-1b"

modelpath = Path("/raid/gdep-guest33/jp_mod/data/models/audio_jp/")
model_out_path = (
  modelpath / f"{pretrained_model_name.replace('/', '_')}_{NUM_EPOCHS}_ft"
)
print(model_out_path)
logdir = Path(
    f"data/logs/audio_jp/{pretrained_model_name.replace('/', '_')}_{NUM_EPOCHS}_ft/"
)

if not os.path.exists(modelpath / "wav2vec2_cb.pth"):
  open(modelpath / "wav2vec2_cb.pth", "w").close()


if not os.path.exists(model_out_path):
    print("Using Hugginface model model as base")
else:
    print("Using older model as base")
    pretrained_model_name = model_out_path

df = load_dataset("common_voice", "ja",split="train")

jsut = DatasetConfig(name="jsut", split="train")
datasets = JAPANESE_DATASETS[1:]
p, df1 = rank0_first(get_datasets, datasets)

DSET_NAMES = "-".join(["cvja"] + [d.name[:4] for d in datasets])
print(DSET_NAMES)

hyperparams = {
    "freeze_feature_extractor": True,
    "num_epochs": 50,
    "datasets": DSET_NAMES,
}

df = pd.DataFrame({"filename": pd.Series(df["path"]), "text": pd.Series(df["sentence"])})
df = pd.concat([df, df1[["filename", "text"]]], ignore_index=True)
if TEST_RUN:
  df = df.iloc[:100]

# %%
"""# Model Training"""


@lru_cache(maxsize=None)
def get_audio_length(s):
    t, sr = torchaudio.load(s)
    return len(t[0]) / sr


from transformers import Wav2Vec2CTCTokenizer

jp_vocab = "/raid/gdep-guest33/itps_transcription_model/vocab.json"

wav2vec2tok = Wav2Vec2CTCTokenizer(
    jp_vocab,
    bos_token="[BOS]",
    eos_token="[EOS]",
    unk_token="[UNK]",
)
node_format_csv = r"%f[7]|"
eos_format_csv = r"[EOS]\n"
unk_format_csv = r"%m|"
m = MeCab.Tagger(
    f" --node-format='{node_format_csv}'"
    + f" --unk-format='{unk_format_csv}'"
    + f" --eos-format='{eos_format_csv}'"
    + " -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd"
)

tok = JPTransformersTokenizer(tok=wav2vec2tok, mcb=m)

tok.decode(tok("こんにちは私の名前はじやんです（笑）"))

df["audio_length"] = df["filename"].apply(get_audio_length)

df["audio_length"].max()

# df["audio_length"].plot.hist()

df["audio_length"].sum() / 60 / 60

df = df[df["audio_length"] < 15].reset_index(drop=True)
df = df[~df["text"].isna()].reset_index(drop=True)

df["audio_length"].sum() / 60 / 60

splits = RandomSplitter(valid_pct=0.2)(df)

tfms = TfmdLists(df, AudioBatchTransform(), splits=splits)
# %% 

model = rank0_first(
    Wav2Vec2ForCTC.from_pretrained,
    # "/content/drive/MyDrive/data/models/audio_jp/",
    pretrained_model_name,
    attention_dropout=0.08,
    hidden_dropout=0.08,
    feat_proj_dropout=0.08,
    mask_time_prob=0.00,
    mask_feature_prob=0.00,
    layerdrop=0.08,
    # ctc_loss_reduction="mean",
    ctc_zero_infinity=True,
    pad_token_id=tok.tokenizer.pad_token_id,
    vocab_size=len(tok.tokenizer),
).cuda()


# %% 
@lru_cache(maxsize=None)
def get_sr(x):
    _, sr = torchaudio.load(x)
    return sr


df["sr"] = df["filename"].apply(get_sr)

df["sr"].unique()

train_text_lens = df.loc[splits[0], "audio_length"].to_list()
val_text_lens = df.loc[splits[1], "audio_length"].to_list()
srtd_dl = partial(SortedDL, res=train_text_lens)
dl_kwargs = [{}, {"val_res": val_text_lens}]

_res = T.Resample(48000, 16000)


@Transform
def resample(x: TensorAudio):
    if x.sr == 48000:
        return TensorAudio(_res(x), sr=16000)
    elif x.sr == 16000:
        return x

# %%

dls = tfms.dataloaders(
    bs=2,
    after_item=[RandomReverbration(p=0.1), resample, tok],
    before_batch=[
        Pad_Audio_Batch(pad_idx_audio=0, pad_idx_text=-100,
         pad_first=True,
         seq_len=1,
         with_attention_masks=True,
         ),
        squeeze,
    ],
    shuffle=True,
    dl_type=srtd_dl,
    dl_kwargs=dl_kwargs,
)

# %%
dls.one_batch()
# %% 


# dls.show_batch(tok=tok, unique=False)


def wer(pred, labels):
    pred_logits = pred.logits
    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)
    pred_str = tok.batch_decode(pred_ids)
    label_str = tok.batch_decode(labels)
    wer = jiwer.wer(label_str, pred_str)
    return wer


def cer(pred, labels):
    pred_logits = pred.logits
    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)
    pred_str = tok.batch_decode(pred_ids)
    label_str = tok.batch_decode(labels)
    cer = jiwer.cer(label_str, pred_str)
    return cer


class TransformersLearner(Learner):
    def _do_one_batch(self):
        self.pred = self.model(
            self.xb[0],
            attention_mask=self.xb[1],
            output_attentions=False,
            labels=cast(self.yb[0], torch.Tensor))
        self("after_pred")
        self.loss_grad = self.pred["loss"]
        self.loss = self.loss_grad.clone()
        self.smooth_loss = self.loss_grad.clone()
        self("after_loss")
        if not self.training or not len(self.yb):
            return
        self("before_backward")
        self.loss_grad.backward()
        self._with_events(self.opt.step, "step", CancelStepException)
        self.opt.zero_grad()


# Commented out IPython magic to ensure Python compatibility.
# gmail
# %env NEPTUNE_API_TOKEN='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1YjMyZmIzZS0zNTEyLTQwNmMtYjUxNy05ZWI5NGY1MjQxMDYifQ=='
# keio.jp
# %env NEPTUNE_API_TOKEN="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0M2FlMmU0Ny0xMTkyLTQ2NTktYWRkYS1lZTcwMmY3ZDEyZjQifQ==",

NEPTUNE = True
if NEPTUNE:
    run = neptune.init("jjs-k/stt-component")

cbs = [
    TensorBoardCallback(log_dir=logdir, trace_model=False, log_preds=False),
]

metrics = [Perplexity(), wer, cer]
learn = TransformersLearner(
    dls,
    model,
    loss_func=noop,  # Loss is calculated in Transformers internally
    metrics=metrics,
    path=Path("."),
    model_dir=modelpath,
    cbs=cbs,
)



def neptune_experiment():
    try:
        print("Got experiment")
        return neptune.get_experiment()
    except Exception:
        print("Created experiment")
        return neptune.create_experiment(
            f"{pretrained_model_name}_{NUM_EPOCHS}E_{DSET_NAMES}",
        )


if FREEZE_FEAT:
    model.freeze_feature_extractor()
if TEST_RUN:
  NUM_EPOCHS=0

sched = {"lr": SchedExp(1e-4, 1e-5)}
fit_cbs = [
    NeptuneCallback(log_model_weights=False, keep_experiment_running=True),
    SeePreds(pretrained_model_name, tok, n_iters=500, log_dir=logdir, neptune=True),
    ParamScheduler(sched),
    SaveModelCallback(
        comp=np.less,
        monitor="wer",
        every_epoch=True,
        at_end=False,
        fname=Path(modelpath / (str(pretrained_model_name).replace("/", "_") + "_cb")),
    ),
    NeptuneSaveModel(1),
    #  SkipToEpoch(LAST_EPOCH)
]
exp = rank0_first(neptune_experiment)
learn.fit(NUM_EPOCHS, cbs=fit_cbs)

if not TEST_RUN:
    if not os.path.exists(model_out_path):
        os.mkdir(model_out_path)
    learn.model.save_pretrained(model_out_path)
    learn.save(Path(model_out_path) / "learn_cb", with_opt=True)
    with open(Path(model_out_path) / "vocab.json", "w") as f:
        json.dump(tok.tokenizer.get_vocab(), f)
    torch.save(learn.model, Path(model_out_path) / "raw_mod_exp.pth")

test_datasets = [
    DatasetConfig(name="jsut", split="test", lang=None, kind=None),
    DatasetConfig(name="nict_spreds", split="test", lang="jp", kind=None),
]
tp, tdf = get_datasets(test_datasets)

tdf["audio_length"] = apply_parallel(tdf["filename"], get_audio_length, 16)
tdf = tdf[tdf["audio_length"] < 15].reset_index(drop=True)
tdf = tdf[~tdf["text"].isna()].reset_index(drop=True)

abt = AudioBatchTransform()
t_tfms = TfmdLists(tdf, abt)

t_tfms = TfmdLists(tdf, abt)

t_dl = dls.new(t_tfms)

learn.validate(dl=t_dl)


def get_preds(xs):
    preds = learn.model(xs)
    pred_logits = preds.logits
    pred_ids = TensorText(np.argmax(pred_logits.detach().cpu().numpy(), axis=-1))
    pred_str = tok.batch_decode(pred_ids)
    return pred_str



comp = [(get_preds(xs), tok.batch_decode(y)) for xs, y in tqdm(iter(t_dl))]

if NEPTUNE:
    for i, (x_pair, y_pair) in enumerate(comp):
        experiment=neptune.get_experiment()
        experiment.log_text(
            f"text_predictions/test_set", f"Targ: {y_pair[0]}"
        )
        experiment.log_text(
            f"text_predictions/test_set", f"Pred: {x_pair[0]}"
        )
        experiment.log_text(
            f"text_predictions/test_set", f"Targ: {y_pair[1]}"
        )
        experiment.log_text(
            f"text_predictions/test_set", f"Pred: {x_pair[1]}"
        )

    neptune.stop()

wers,cers = [], []
for xs, y in tqdm(iter(t_dl)):
    wers.append(wer(learn.model(xs), y))
    cers.append(cer(learn.model(xs), y))
wers, cers = np.array(wers), np.array(cers)

print("wer performance on test dataset: ", wers.mean())
print("cer performance on test dataset: ", cers.mean())

# %%
