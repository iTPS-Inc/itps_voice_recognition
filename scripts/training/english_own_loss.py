# -*- coding: utf-8 -*-
"""english_adapted_jp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iEu8K0cyOMCWNIhVD68lfqR5J_-k1Gt1

# Setup
"""

# from google.colab import drive
# drive.mount('/content/drive')
# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cd /content/
# if [ -d itps_voice_recognition ]; then
#   cd itps_voice_recognition
#   git pull -r
#   cd ..
# else
#   git clone https://github.com/iTPS-Inc/itps_voice_recognition.git
# fi
#
# if [ ! -f installed ]; then
#
# pip install boto3 datasets==1.13.3 transformers==4.11.3 librosa jiwer sentencepiece japanize_matplotlib
# pip install neptune-client mecab-python3
# pip install fastai -Uqq
#
# touch installed
# fi

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %load_ext tensorboard
# %autoreload 2

import os

from dsets.dset_config.dset_config import DatasetConfig

# os.chdir("/content/itps_voice_recognition")
from dsets.dsets import ENGLISH_DATASETS, get_datasets
from dsets.helpers.helpers import apply_parallel
from itpsaudio.core import *
from itpsaudio.modelling import SmoothCTCLoss
from itpsaudio.transforms import *
from itpsaudio.aug_transforms import (
    FrequencyMaskAugment,
    TimeMaskAugment,
    ToSpec,
    ToWave,
)


"""# Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir '/content/drive/MyDrive/data/logs/audio_en'

"""# Imports

## Imports
"""

import json
import pickle
from functools import lru_cache
from pathlib import Path

import jiwer
import neptune as neptune
import torchaudio
import torchaudio.transforms as T
from fastai.callback.neptune import *
from fastai.callback.tensorboard import *
from fastai.data.all import *
from fastai.text.all import *
from fastai.vision.all import *
from itpsaudio.aug_transforms import (
    FrequencyMaskAugment,
    RandomReverbration,
    TimeMaskAugment,
)
from itpsaudio.callbacks import SeePreds
from transformers import AutoModelForCTC, Wav2Vec2CTCTokenizer

"""## Parameters"""

dset_lengths = None
TEST_RUN = False
LAST_EPOCH = 0
NUM_EPOCHS = 30
AUDIO_LENGTH = 10
FREEZE_FEAT = True

# pretrained_model_name = "facebook/wav2vec2-base"
pretrained_model_name = "facebook/hubert-xlarge-ll60k"

modelpath = Path("/content/drive/MyDrive/data/models/audio_en/")
model_out_path = f"/content/drive/MyDrive/data/models/audio_en/{pretrained_model_name.replace('/', '_')}_{NUM_EPOCHS}_aug_smooth"
logdir = Path(
    f"/content/drive/MyDrive/data/logs/audio_en/{pretrained_model_name.replace('/', '_')}_{NUM_EPOCHS}_aug_smooth/"
)
jp_vocab = "/content/drive/MyDrive/data/models/audio_en/en_vocab_file.json"

if not os.path.exists(model_out_path):
    print("Using Hugginface model model as base")
else:
    print("Using older model as base")
    pretrained_model_name = model_out_path
    print(pretrained_model_name)


jsut = DatasetConfig(name="jsut", split="train")
# datasets= [jsut]
datasets = [
    #  DatasetConfig(name='itps', split='train', lang='en', kind=None),
    DatasetConfig(name="librispeech", split="train", lang=None, kind="clean"),
    DatasetConfig(name="ljl", split="train", lang=None, kind=None),
    DatasetConfig(name="nict_spreds", split="train", lang="en", kind=None),
]

p, df = get_datasets(datasets)

DSET_NAMES = "-".join(["cven"] + [d.name[:4] for d in datasets])
print(DSET_NAMES)
hyperparams = {
    "freeze_feature_extractor": True,
    "num_epochs": 50,
    "datasets": DSET_NAMES,
}
if TEST_RUN:
    df = df.iloc[:100]

"""# Model Training"""


@lru_cache(maxsize=None)
def get_audio_length(s):
    t, sr = torchaudio.load(s)
    return len(t[0]) / sr, sr


def prepare_df(df, audio_length=10):
    df[["audio_length", "sr"]] = df["filename"].apply(
        lambda x: pd.Series(get_audio_length(x))
    )
    print("Longest clip: ", df["audio_length"].max())
    df["audio_length"].plot.hist()
    plt.show()
    print("Length of datset before filtering:", df["audio_length"].sum() / 60 / 60)
    df = df[df["audio_length"] < audio_length].reset_index(drop=True)
    df = df[~df["text"].isna()].reset_index(drop=True)
    print("Length of dataset after filtering: ", df["audio_length"].sum() / 60 / 60)
    df["audio_length"].plot.hist()
    return df


if not os.path.exists("/content/df.pkl"):
    df = prepare_df(df, audio_length=AUDIO_LENGTH)
    df.to_pickle("/content/df.pkl")
else:
    df = pd.read_pickle("/content/df.pkl")

splits = RandomSplitter(valid_pct=0.2)(df)

tfms = TfmdLists(df, AudioBatchTransform(), splits=splits)

df["sr"].unique()
samplers = {i: T.Resample(i) for i in df["sr"].unique()}

train_text_lens = df.loc[splits[0], "audio_length"].to_list()
val_text_lens = df.loc[splits[1], "audio_length"].to_list()
srtd_dl = partial(SortedDL, res=train_text_lens)
dl_kwargs = [{}, {"val_res": val_text_lens}]


@Transform
def resample(x: TensorAudio):
    if x.sr == 16000:
        return x
    else:
        return samplers[x.sr](x)


@Transform
def prepare_for_tokenizer(x: str):
    return x.upper().replace(" ", "|")


tok = ENTransformersTokenizer(
    tok=Wav2Vec2CTCTokenizer(
        jp_vocab,
        bos_token="[BOS]",
        eos_token="[EOS]",
        unk_token="[UNK]",
        pad_token="[PAD]",
    )
)

dls = tfms.dataloaders(
    bs=4,
    after_item=[
        RandomReverbration(p=0.1),
        resample,
        ToSpec(),
        FrequencyMaskAugment(p=0.2),
        TimeMaskAugment(p=0.2),
        ToWave(),
        prepare_for_tokenizer,
        tok,
    ],
    before_batch=[
        Pad_Audio_Batch(
            pad_idx_audio=0,
            pad_idx_text=0,
            pad_first=True,
            seq_len=1,
            with_attention_masks=True,
        ),
        squeeze,
    ],
    shuffle=True,
    n_inp=1,
    dl_type=srtd_dl,
    dl_kwargs=dl_kwargs,
)

dls.one_batch()

dls.show_batch(tok=tok, unique=False)

"""# Metrics, Learner """


def wer(pred, labels):
    pred_logits = pred.logits
    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)
    pred_str = tok.batch_decode(pred_ids)
    label_str = tok.batch_decode(labels)
    wer = jiwer.wer(label_str, pred_str)
    return wer


def cer(pred, labels):
    pred_logits = pred.logits
    pred_ids = np.argmax(pred_logits.detach().cpu().numpy(), axis=-1)
    pred_str = tok.batch_decode(pred_ids)
    label_str = tok.batch_decode(labels)
    cer = jiwer.cer(label_str, pred_str)
    return cer


class TransformersLearner(Learner):
    def _do_one_batch(self):
        self.pred = self.model(self.xb[0], attention_mask=self.xb[1])
        self("after_pred")
        if len(self.yb):
            self.loss_grad = self.loss_func(
                preds=cast(self.pred.logits, torch.Tensor),
                inp_len=self.xb[1].sum(1).long(),
                labels=cast(self.yb[0], torch.Tensor),
                modelconf=self.model.config,
            )
            self.loss = self.loss_grad.clone()
        self("after_loss")
        if not self.training or not len(self.yb):
            return
        self("before_backward")
        self.loss_grad.backward()
        self._with_events(self.opt.step, "step", CancelStepException)
        self.opt.zero_grad()


"""# Training"""

NEPTUNE = True
if NEPTUNE:
    run = neptune.init("jjs/itps-language-model")

monitor = "wer"
cbs = [
    TensorBoardCallback(log_dir=logdir, trace_model=False, log_preds=False),
    SaveModelCallback(
        comp=np.less,
        monitor=monitor,
        every_epoch=True,
        at_end=False,
        fname=Path(modelpath / "cb"),
    ),
]

metrics = [Perplexity(), wer, cer]

learn = TransformersLearner(
    dls=dls,
    model=AutoModelForCTC.from_pretrained(
        # "/content/drive/MyDrive/data/models/audio_jp/",
        pretrained_model_name,
        attention_dropout=0.15,
        hidden_dropout=0.15,
        feat_proj_dropout=0.15,
        layerdrop=0.15,
        ctc_zero_infinity=True,
        pad_token_id=0,
        vocab_size=len(tok.tokenizer),
    ),
    loss_func=SmoothCTCLoss(len(tok.tokenizer)),
    metrics=metrics,
    model_dir=modelpath,
    cbs=cbs,
)

start_lr = 1e-7
end_lr = 10
r = learn.lr_find(
    start_lr=start_lr, end_lr=end_lr, num_it=100, stop_div=True, suggest_funcs=()
)

if NEPTUNE:
    log_cbs = [
        NeptuneCallback(log_model_weights=False, keep_experiment_running=False),
        NeptuneSaveModel(1),
    ]
    neptune.create_experiment(f"{pretrained_model_name}_{NUM_EPOCHS}E_{DSET_NAMES}")

if FREEZE_FEAT:
    learn.model.freeze_feature_extractor()
if TEST_RUN:
    learn.fit_one_cycle(1, lr_max=1e-4)
else:
    sched = {"lr": SchedExp(1e-4, 1e-5)}
    fit_cbs = [
        SeePreds(
            pretrained_model_name, tok, n_iters=500, log_dir=logdir, neptune=NEPTUNE
        ),
        ParamScheduler(sched),
        SaveModelCallback(fname=pretrained_model_name.replace("/", "_")),
    ]
    learn.fit(NUM_EPOCHS, cbs=fit_cbs)

    if not os.path.exists(model_out_path):
        os.mkdir(model_out_path)
    learn.save(Path(model_out_path) / "learn_cb", with_opt=True)

if NEPTUNE:
    neptune.stop()

if not TEST_RUN:
    learn.model.save_pretrained(model_out_path)
    learn.save(Path(model_out_path) / "learner.falearner")
    with open(Path(model_out_path) / "vocab.json", "w") as f:
        json.dump(tok.tokenizer.get_vocab(), f)
    torch.save(learn.model, Path(model_out_path) / "raw_mod_exp.pth")

test_datasets = [
    DatasetConfig(name="itps", lang="jp", split="test"),
    DatasetConfig(name="jsut", split="test", lang=None, kind=None),
    DatasetConfig(name="nict_spreds", split="test", lang="jp", kind=None),
]

tp, tdf = get_datasets(test_datasets)

tdf["audio_length"] = apply_parallel(tdf["filename"], get_audio_length, 16)
tdf = tdf[tdf["audio_length"] < 15].reset_index(drop=True)
tdf = tdf[~tdf["text"].isna()].reset_index(drop=True)

abt = AudioBatchTransform()
t_tfms = TfmdLists(tdf, abt)

if TEST_RUN:
    t_tfms = TfmdLists(tdf.iloc[:100], abt)
else:
    t_tfms = TfmdLists(tdf, abt)

t_dl = dls.new(t_tfms)

t_dl.show_batch(tok=tok)


def get_preds(xs):
    preds = learn.model(xs)
    pred_logits = preds.logits
    pred_ids = TensorText(np.argmax(pred_logits.detach().cpu().numpy(), axis=-1))
    pred_str = tok.batch_decode(pred_ids)
    return pred_str


import pprint

for xs, _, _, y in iter(t_dl):
    print(wer(learn.model(xs), y))
    print(cer(learn.model(xs), y))
    pprint.pprint(dict(enumerate(list(zip(get_preds(xs), tok.batch_decode(y))))))
    break

comp = [(get_preds(xs), tok.batch_decode(y)) for xs, y in iter(t_dl)]

learn.model

for i, (x_pair, y_pair) in enumerate(comp):

    print("Pred: ", x_pair[0])
    print("Targ: ", y_pair[0])
    print("Pred: ", x_pair[1])
    print("Targ: ", y_pair[1])
    if (i + 1 % 10) == 0:
        break

with open("/content/drive/MyDrive/data/models/audio_jp/good_jp_mod/tok.pkl", "rb") as f:
    t = pickle.load(f)
